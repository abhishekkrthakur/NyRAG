# LLM API configuration (required for chat answers)
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=your-llm-api-key-here
LLM_MODEL=openai/gpt-5.1

# Vespa connection (for local deployment)
# VESPA_URL=http://localhost
# VESPA_PORT=8080
# VESPA_CONFIGSERVER_URL=http://localhost:19071

# Vespa Cloud (required when deploy_mode: cloud in config)
# VESPA_CLOUD_TENANT=your-tenant
# VESPA_CLOUD_APPLICATION=your-app  # optional; defaults to generated app name (e.g. nyrag<projectname>)
# VESPA_CLOUD_INSTANCE=default
# VESPA_CLOUD_API_KEY_PATH=/path/to/api-key.pem
# VESPA_CLOUD_API_KEY="-----BEGIN PRIVATE KEY-----..."

# Optional mTLS for Vespa endpoints (required for Vespa Cloud dataplane)
# VESPA_CLIENT_CERT=/path/to/data-plane-public-cert.pem
# VESPA_CLIENT_KEY=/path/to/data-plane-private-key.pem
# VESPA_CA_CERT=/path/to/ca.pem
# VESPA_TLS_VERIFY=1

# Docker compose deployment (set to 1 when running inside docker-compose)
# NYRAG_VESPA_COMPOSE=1

# Python settings
PYTHONUNBUFFERED=1
